== Server administration

=== System requirements

==== Hardware requirements

* Metadb software:
** CPU: 2 cores
** Memory: 1 GB
** Storage: 500 GB SSD
* PostgreSQL database:
** CPU: 4 cores
** Memory: 32 GB
** Storage: 2 TB SSD

==== Software requirements

* Architecture: x86-64 (AMD64)
* Operating system: Debian 11 or later
* https://www.postgresql.org/[PostgreSQL] 15 or later
** PostgreSQL 15.2 or later is recommended
** AWS RDS PostgreSQL optionally may be used (with servers in the same
   zone/subnet); Aurora is not supported
* Required to build from source:
** https://golang.org/[Go] 1.20 or later
** goyacc (installation instructions below)
** https://www.colm.net/open-source/ragel/[Ragel] 6.10 or later
** https://gcc.gnu.org/[GCC C compiler] 9.3.0 or later

==== PostgreSQL configuration

* `checkpoint_timeout`: `3600`
* `cpu_tuple_cost`: `0.03`
* `default_statistics_target`: `1000`
* `effective_io_concurrency`: `200`
* `idle_in_transaction_session_timeout`: `3600000`
* `idle_session_timeout`: `604800000`
* `maintenance_work_mem`: `1000000`
* `max_wal_size`: `10240`
* `shared_buffers`: `1250000`
* `statement_timeout`: `7200000`
* `work_mem`: `350000`

=== Building the software

First set the `GOPATH` environment variable to specify a path that can serve as
the build workspace for Go, e.g.:

[source,bash]
----
$ export GOPATH=$HOME/go
----

Then install goyacc:

[source,bash]
----
go install golang.org/x/tools/cmd/goyacc@master

export PATH=$PATH:$GOPATH/bin
----

Then to build the server:

[source,bash]
----
$ ./build.sh
----

The `build.sh` script creates a `bin/` subdirectory and builds the `metadb`
executable there:

[source,bash]
----
$ ./bin/metadb help
----

It is suggested that a `metadb` user be created and the server run by that
user, for example, in `/home/metadb`.

=== Server configuration

Metadb makes use of local storage in a *data directory* which is created using
`metadb` with the `init` command.  In this example we will create the data
directory as `data/`:

[source,bash]
----
metadb init -D data
----

This will also create a file `metadb.conf` in the data directory with the
following structure:

[source,subs="verbatim,quotes"]
----
[main]
host = _host name of the PostgreSQL server_
port = _port number of the PostgreSQL server_
database = _database name_
superuser = _database superuser such as "postgres"_
superuser_password = _password of superuser_
systemuser = _database user that is the owner of the database_
systemuser_password = _password of systemuser_
sslmode = _should be set to "require" or stronger option_
----

The database name should be `metadb` or should begin with `metadb_`.

This file should be edited to add database connection parameters, for example:

[source,subs="verbatim,quotes"]
----
[main]
host = a.b.c
port = 5432
database = metadb
superuser = postgres
superuser_password = zpreCaWS7S79dt82zgvD
systemuser = mdbadmin
systemuser_password = ZHivGie5juxGJZmTObHU
sslmode = require
----

Metadb will assume that the database, superuser, and systemuser defined here
already exist; so they should be created before continuing.

=== Backups

*It is essential to make regular backups of Metadb and to test the backups.*

In general, persistent data are stored in the database, and therefore the
database is the most important system that should be backed up often.

The data directory contains the `metadb.conf` configuration file and is also
used for temporary storage.  The `metadb.conf` file should be backed up, or
alternatively it should be possible to reconstruct the file's contents.

=== Upgrading from a previous version

To upgrade from any previous version of Metadb, stop the server (if running),
and then run the upgrade process in case changes are required:

----
metadb upgrade -D data
----

The upgrade process may, in some instances, take a significant amount of time
to run.  The database generally remains available to users during this period.

If no changes are needed, the process outputs:

----
metadb: "data" is up to date
----

=== Running the server

To start the server:

[source,bash]
----
nohup metadb start -D data -l metadb.log &
----

The `--memlimit` option can be used to set a soft memory limit (in GiB) if
needed, for example:

[source,bash]
----
nohup metadb start -D data -l metadb.log --memlimit 2 &
----

The server listens on port 8440 by default, but this can be set using the
`--port` option.  The `--debug` option enables verbose logging.

To stop the server:

[source,bash]
----
metadb stop -D data
----

Note that stopping or restarting the server may delay scheduled data updates or
cause them to restart.

The server can be set up to run with systemd via a file such as
`/etc/systemd/system/metadb.service`, for example:

[source,ini]
----
[Unit]
Description=Metadb
After=network.target remote-fs.target

[Service]
Type=simple
User=metadb
ExecStart=/bin/bash -ce "exec /home/metadb/bin/metadb start -D /home/metadb/data -l /home/metadb/metadb.log"
Restart=on-abort

[Install]
WantedBy=multi-user.target
----

Then:

----
systemctl enable metadb

systemctl start metadb
----

=== Connecting to the server

The PostgreSQL terminal client, `psql`, can be used to connect to the Metadb
server:

----
psql -h localhost -p 8440
----

Then for example to show the software version:

----
SELECT mdbversion();
----

=== Configuring a Kafka data source

A data source is defined using the `CREATE DATA SOURCE` statement, for example:

----
CREATE DATA SOURCE sensor TYPE kafka OPTIONS (
    brokers 'kafka:29092',
    topics '^metadb_sensor_1.*',
    consumergroup 'metadb_sensor_1_1',
    addschemaprefix 'sensor_',
    schemastopfilter 'admin'
);
----

Metadb currently supports reading Kafka messages in the format produced by the
Debezium PostgreSQL connector for Kafka Connect.  Configuration of Kafka, Kafka
Connect, Debezium, and PostgreSQL logical decoding is beyond the scope of this
documentation, but a few notes are included here.

Data flow in this direction:

1. A source PostgreSQL database
2. Kafka Connect/Debezium
3. Kafka
4. Metadb
5. The analytics database

To allow capturing data changes in the source PostgreSQL database, logical
decoding has to be enabled, in particular by setting `wal_level = logical` in
`postgresql.conf`.

Next we create a connector configuration file for Kafka Connect:

----
{
    "name": "sensor-1-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.dbname": "sourcedb",
        "database.hostname": "example.host.name",
        "database.password": "eHrkGrZL8mMJOFgToqqL",
        "database.port": "5432",
        "database.server.name": "metadb_sensor_1",
        "database.user": "dbuser",
        "plugin.name": "pgoutput",
        "snapshot.mode": "exported",
        "tasks.max": "1",
        "truncate.handling.mode": "include",
        "publication.autocreate.mode", "filtered"
        "heartbeat.interval.ms": "30000",
        "heartbeat.action.query": "UPDATE admin.heartbeat set last_heartbeat_ts = now();"
    }
}
----

It is recommended to use the connector configuration settings
`heartbeat.interval.ms` and `heartbeat.action.query` as above to avoid spikes
in disk space consumption within the source database.  (See the Debezium
PostgreSQL connector documentation for more details.)  The `schemastopfilter`
option of the `CREATE DATA SOURCE` command is used to filter out the heartbeat
table.

In the source database:

----
CREATE SCHEMA admin;

CREATE TABLE admin.heartbeat (last_heartbeat_ts timestamptz PRIMARY KEY);

INSERT INTO admin.heartbeat (last_heartbeat_ts) VALUES (now());
----

Then to create the connector:

----
curl -X POST -i -H "Accept: application/json" -H "Content-Type: application/json" \
     -d @connector.json https://kafka.connect.server/connectors
----

Note the `1` included in `name` and `database.server.name` in the connector
configuration.  This is suggested as a version number, which can be incremented
if the data stream needs to be resynchronized with a new connector.

Metadb requires all streamed tables to have a primary key defined or a replica
identity of `FULL`.  Tables that do not meet this requirement should be
filtered out in the Debezium PostgreSQL connector configuration by setting
`schema.exclude.list` or `table.exclude.list`.  Otherwise they will generate
error messages in the Metadb log.

*The replication slot disk usage must be monitored, because under certain error
conditions it can grow too large and possibly fill up the disk.* To show the
disk usage (in the source database):

----
select slot_name, pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(),
    restart_lsn)) as replicationSlotLag, active from pg_replication_slots;
----

*It is also recommended to allocate plenty of extra disk space in the source
database.*

To drop the replication slot (in the source database) after deleting a
connector:

----
SELECT pg_drop_replication_slot('debezium');

DROP PUBLICATION dbz_publication;
----

=== Resynchronizing a data source

If a Kafka data stream fails and cannot be resumed, it may be necessary to
re-stream data to Metadb.  For example, a source database may become
unsynchronized with the analytic database, requiring a new snapshot of the
source database to be streamed.  Metadb can accept re-streamed data in order to
resynchronize with the source, using the following procedure.

Note that during the resynchronization process, the analytics database
continues to be available to users.  However, streaming updates will be slower
than usual, and there temporarily may be missing records (until they are
re-streamed) or "extra" records (recently deleted in the source database).
Also, periodic transforms and external SQL are paused during resynchronization.

1. Update the `topics` and `consumergroup` configuration settings for the new
   data stream.
+
[source]
----
ALTER DATA SOURCE sensor OPTIONS
    (SET topics '^metadb_sensor_2.*', SET consumergroup 'metadb_sensor_2_1');
----
+
*Do not restart the Metadb server but continue directly to Step 2.*

2. Stop the Metadb server and (before starting it again) "reset" the analytics
   database to mark all current data as old.  This may take some time to run.
+
[source,bash]
----
metadb stop -D data

metadb reset -D data --source sensor
----

3. Start the Metadb server to begin streaming the data.

4. Once the new data have finished (or nearly finished) re-streaming, stop the
   Metadb server, and "clean" the analytics database to remove any old data
   that have not been refreshed by the new data stream.
+
[source,bash]
----
metadb clean -D data --source sensor
----
+
The timing of when "metadb clean" should be run is up to the admninistrator,
but *it must be run to complete the resynchronization process*.  In most cases
it will be more convenient for users if "metadb clean" is run too late
(delaying removal of deleted records) rather than too early (removing records
before they have been restreamed).
+
[.aqua-background]#Metadb v1.0.10#
Metadb detects when snapshot data are no longer being received, and then writes
to the log a message such as "resync snapshot complete (deadline exceeded)."
This generally means it is a good time to run "metadb clean".

5. Start the server.
+
Until a failed stream is re-streamed by following the process above, the
analytic database may continue to be unsynchronized with the source.

=== Creating database users

To create a new database user account:

[source]
----
CREATE USER weg PASSWORD 'LZn2DCajcNHpGR3ZXWHD';
----

In addition to creating the user, this also creates a schema with the same name
as the user.  The schema is intended as a workspace for the user.

.Recommendations:
* Each user account should be for an individual user and not shared by more
  than one person.
* Prefer user names of 3 to 8 characters in length.

By default the user does not have access to data tables.  To give the user
access to all tables generated from a data source (including tables not yet
created):

[source]
----
AUTHORIZE SELECT ON ALL TABLES IN DATA SOURCE sensor TO weg;
----

.Note
****
[.text-center]
AUTHORIZE currently requires restarting the server before it will take
effect.
****

=== Administrative database changes

It is possible to make administrative-level changes directly in the underlying
PostgreSQL database, such as providing additional tables for users.  However,
the following guidelines should be followed strictly to avoid disrupting the
operation of Metadb:

1. No changes should be made to any database objects created by Metadb.  If it
should become necessary to make changes to the database objects at the request
of the Metadb maintainers, the server should be stopped first to prevent it
from operating with an out-of-date cache.  If changes are made to the database
objects inadvertently, the server should be stopped as soon as possible and not
started until the changes have been reversed.

2. Changes generally should be made while logged in as a new database user (not
a superuser) that is created specifically for that purpose.

3. Any new database objects should be created in a new schema that will not
coincide with a schema that may be created by Metadb.  This can be ensured by
always setting `addschemaprefix` in data source configurations and avoiding
names with those prefixes when creating a new schema.

4. Database views should not be created in the database.
