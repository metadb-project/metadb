== Server administration

=== System requirements

* Architecture: x86-64 (AMD64)
* Operating system: Debian 11
* https://www.postgresql.org/[PostgreSQL] 14.5 or later
* Required to build from source:
** https://golang.org/[Go] 1.19 or later
** goyacc (installation instructions below)
** https://www.colm.net/open-source/ragel/[Ragel] 6.10 or later
** https://gcc.gnu.org/[GCC C compiler] 9.3.0 or later

=== Building the software

First set the `GOPATH` environment variable to specify a path that can
serve as the build workspace for Go, e.g.:

[source,bash]
----
$ export GOPATH=$HOME/go
----

Then install goyacc:

[source,bash]
----
go install golang.org/x/tools/cmd/goyacc@master

export PATH=$PATH:$GOPATH/bin
----

Then to build the server:

[source,bash]
----
$ ./build.sh
----

The `build.sh` script creates a `bin/` subdirectory and builds the
`metadb` executable there:

[source,bash]
----
$ ./bin/metadb help
----

It is suggested that a `metadb` user be created and the server run by
that user, for example, in `/home/metadb`.

=== Server configuration

Metadb makes use of local storage in a "data directory," which is
created using `metadb` with the `init` command.  In this example we
will create the data directory as `data/`:

[source,bash]
----
metadb init -D data
----

This will also create a file `metadb.conf` in the data directory:

[source,toml]
----
[main]
host =
port =
database =
superuser =
superuser_password =
systemuser =
systemuser_password =
sslmode =
----

This file should be edited to add connection parameters, for example:

[source,toml]
----
[main]
host = a.b.c
port = 5432
database = mydb
superuser = postgres
superuser_password = zpreCaWS7S79dt82zgvD
systemuser = metadb
systemuser_password = ZHivGie5juxGJZmTObHU
sslmode = require
----

Metadb will assume that the database, superuser, and systemuser
defined here already exist; so they should be created before
continuing.

It is also recommended to revoke `CREATE` privileges on the `public`
schema.

----
REVOKE CREATE ON SCHEMA public FROM public;
----

=== Running the server

To start the server:

[source,bash]
----
nohup metadb start -D data -l metadb.log &
----

The server listens on port 8440 by default.  The `--debug` option can
be used to enable verbose logging.

To stop the server:

[source,bash]
----
metadb stop -D data
----

The server can be set up to run with systemd via a file such as
`/etc/systemd/system/metadb.service`, for example:

[source,ini]
----
[Unit]
Description=Metadb
After=network.target remote-fs.target

[Service]
Type=simple
User=metadb
ExecStart=/bin/bash -ce "exec /home/metadb/bin/metadb start -D /home/metadb/data -l /home/metadb/metadb.log"
Restart=on-abort

[Install]
WantedBy=multi-user.target
----

Then:

----
systemctl enable metadb

systemctl start metadb
----

=== Connecting to the server

The PostgreSQL terminal client, `psql`, can be used to connect to the
Metadb server:

----
psql -h localhost -p 8440
----

Then for example to show the software version:

----
SELECT metadb_version();
----

=== Configuring a Kafka data source

A data source is defined using the `CREATE DATA SOURCE` statement, for
example:

----
CREATE DATA SOURCE example TYPE kafka OPTIONS (
    brokers 'kafka:29092',
    topics '^metadb_example_1.*',
    consumergroup 'metadb_example_1_1',
    addschemaprefix 'example_'
);
----

Metadb currently supports reading Kafka messages produced by the
Debezium PostgreSQL connector for Kafka Connect.  Configuration of
Kafka, Kafka Connect, Debezium, and PostgreSQL logical decoding is
beyond the scope of this documentation, but a few notes are included
here.

Data flows from (1) a source PostgreSQL database to (2) the Debezium
PostgreSQL connector in Kafka Connect to (3) Kafka to (4) Metadb and
its PostgreSQL database.

To allow capturing data changes in the source PostgreSQL database,
logical decoding must be enabled, in particular by setting `wal_level
= logical` in `postgresql.conf`.

Next, to begin streaming to Kafka, "POST" a connector configuration to
Kafka Connect at `/connectors`, for example creating a file
`connector.json`:

----
{
    "name": "example-1-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.dbname": "sourcedb",
        "database.hostname": "example.host.name",
        "database.password": "eHrkGrZL8mMJOFgToqqL",
        "database.port": "5432",
        "database.server.name": "metadb_example_1",
        "database.user": "dbuser",
        "plugin.name": "pgoutput",
        "snapshot.mode": "exported",
        "tasks.max": "1",
        "truncate.handling.mode": "include",
        "publication.autocreate.mode", "filtered"
    }
}
----

Then:

----
curl -X POST -i -H "Accept: application/json" -H "Content-Type: application/json" \
     -d @connector.json https://kafka.connect.server/connectors
----

Note the `1` included in `name` and `database.server.name` in the
connector configuration.  This is suggested as a version number, which
can be incremented if the data stream needs to be resynchronized with
a new connector.

Metadb requires all streamed tables to have a primary key defined or a
replica identity of `FULL`.  Tables that do not meet this requirement
should be filtered out in the Debezium PostgreSQL connector
configuration by setting `schema.exclude.list` or
`table.exclude.list`.  Otherwise they will generate error messages in
the Metadb log.

It is recommended to use the `heartbeat.action.query` connector
configuration to avoid spikes in disk space consumption within the
source database.  (See the Debezium PostgreSQL connector documentation
for more details.)

The replication slot disk usage must be monitored, because under
certain error conditions it can grow too large and possibly fill up
the disk.  To show the disk usage (in the source database):

----
select slot_name, pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(),
    restart_lsn)) as replicationSlotLag, active from pg_replication_slots;
----

To drop the replication slot (in the source database) after deleting a
connector:

----
SELECT pg_drop_replication_slot('debezium');

DROP PUBLICATION dbz_publication;
----

=== Resynchronizing a data source

If a Kafka data stream fails and cannot be resumed, it may be
necessary to re-stream data to Metadb.  For example, a source database
may become unsynchronized with the analytic database, requiring a new
snapshot of the source database to be streamed.  Metadb can accept
re-streamed data in order to resynchronize with the source, using this
procedure:

1. Update the `topics` and `consumergroup` configuration settings for
   the new data stream.
+
[source]
----
ALTER DATA SOURCE example OPTIONS
    (SET topics '^metadb_example_2.*', SET consumergroup 'metadb_example_2_1');
----

2. Stop the Metadb server, and then "reset" the analytic database to
   mark current data as old.  This may take some time to run.
+
[source,bash]
----
metadb stop -D data

metadb reset -D data --source example
----

3. Start the Metadb server to begin streaming the data.

4. Once the new data have finished (or nearly finished) re-streaming,
   stop the Metadb server, and "clean" the analytic database to remove
   old data.
+
[source,bash]
----
metadb clean -D data --source example
----
+
Note that the metadb server currently does not give an indication that
it has finished re-streaming, except that running it with `--debug`
will typically show updates slowing down.  (Having the server report
that initial streaming or re-streaming has finished is a planned
feature.)
+
The precise timing of when "metadb clean" is run is not important, but
it must be run to remove redundant data and to complete the
resynchronization process.

5. Start the server.
+
Until a failed stream is re-streamed by following the process above,
the analytic database may continue to be unsynchronized with the
source.
