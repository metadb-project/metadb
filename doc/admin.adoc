== Server administration

=== System requirements

==== Hardware requirements

* Metadb software:
** CPU: 2 cores
** Memory: 2 GB
** Storage: 500 GB SSD
* PostgreSQL database:
** CPU: 4 cores
** Memory: 32 GB
** Storage: 2 TB SSD

==== Software requirements

* Architecture: x86-64 (AMD64)
* Operating system: Debian 11
* https://www.postgresql.org/[PostgreSQL] 14 or later
** PostgreSQL 15 or later is recommended
** AWS RDS PostgreSQL optionally may be used (with servers in the same
   zone/subnet); Aurora is not supported
* Required to build from source:
** https://golang.org/[Go] 1.19 or later
** goyacc (installation instructions below)
** https://www.colm.net/open-source/ragel/[Ragel] 6.10 or later
** https://gcc.gnu.org/[GCC C compiler] 9.3.0 or later

==== PostgreSQL configuration

* `checkpoint_timeout`: `3600`
* `cpu_tuple_cost`: `0.03`
* `default_statistics_target`: `1000`
* `effective_io_concurrency`: `200`
* `idle_in_transaction_session_timeout`: `3600000`
* `idle_session_timeout`: `604800000`
* `maintenance_work_mem`: `1000000`
* `max_wal_size`: `10240`
* `shared_buffers`: `1250000`
* `statement_timeout`: `3600000`
* `work_mem`: `350000`

=== Building the software

First set the `GOPATH` environment variable to specify a path that can
serve as the build workspace for Go, e.g.:

[source,bash]
----
$ export GOPATH=$HOME/go
----

Then install goyacc:

[source,bash]
----
go install golang.org/x/tools/cmd/goyacc@master

export PATH=$PATH:$GOPATH/bin
----

Then to build the server:

[source,bash]
----
$ ./build.sh
----

The `build.sh` script creates a `bin/` subdirectory and builds the
`metadb` executable there:

[source,bash]
----
$ ./bin/metadb help
----

It is suggested that a `metadb` user be created and the server run by
that user, for example, in `/home/metadb`.

=== Server configuration

Metadb makes use of local storage in a "data directory," which is
created using `metadb` with the `init` command.  In this example we
will create the data directory as `data/`:

[source,bash]
----
metadb init -D data
----

This will also create a file `metadb.conf` in the data directory:

[source,toml]
----
[main]
host =
port =
database =
superuser =
superuser_password =
systemuser =
systemuser_password =
sslmode =
----

This file should be edited to add connection parameters, for example:

[source,toml]
----
[main]
host = a.b.c
port = 5432
database = mydb
superuser = postgres
superuser_password = zpreCaWS7S79dt82zgvD
systemuser = metadb
systemuser_password = ZHivGie5juxGJZmTObHU
sslmode = require
----

Metadb will assume that the database, superuser, and systemuser
defined here already exist; so they should be created before
continuing.

=== Running the server

If upgrading from a previous version, first run the upgrade process in
case changes are needed:

----
metadb upgrade -D data
----

Then to start the server:

[source,bash]
----
nohup metadb start -D data -l metadb.log &
----

The `--memlimit` option can be used to set a soft memory limit (in
GiB) if needed, for example:

[source,bash]
----
nohup metadb start -D data -l metadb.log --memlimit 2 &
----

The server listens on port 8440 by default, but this can be set using
the `--port` option.  The `--debug` option enables verbose logging.

To stop the server:

[source,bash]
----
metadb stop -D data
----

The server can be set up to run with systemd via a file such as
`/etc/systemd/system/metadb.service`, for example:

[source,ini]
----
[Unit]
Description=Metadb
After=network.target remote-fs.target

[Service]
Type=simple
User=metadb
ExecStart=/bin/bash -ce "exec /home/metadb/bin/metadb start -D /home/metadb/data -l /home/metadb/metadb.log"
Restart=on-abort

[Install]
WantedBy=multi-user.target
----

Then:

----
systemctl enable metadb

systemctl start metadb
----

=== Connecting to the server

The PostgreSQL terminal client, `psql`, can be used to connect to the
Metadb server:

----
psql -h localhost -p 8440
----

Then for example to show the software version:

----
SELECT metadb_version();
----

=== Configuring a Kafka data source

A data source is defined using the `CREATE DATA SOURCE` statement, for
example:

----
CREATE DATA SOURCE example TYPE kafka OPTIONS (
    brokers 'kafka:29092',
    topics '^metadb_example_1.*',
    consumergroup 'metadb_example_1_1',
    addschemaprefix 'example_',
    schemastopfilter 'admin'
);
----

Metadb currently supports reading Kafka messages produced by the
Debezium PostgreSQL connector for Kafka Connect.  Configuration of
Kafka, Kafka Connect, Debezium, and PostgreSQL logical decoding is
beyond the scope of this documentation, but a few notes are included
here.

Data flows from (1) a source PostgreSQL database to (2) the Debezium
PostgreSQL connector in Kafka Connect to (3) Kafka to (4) Metadb and
its PostgreSQL database.

To allow capturing data changes in the source PostgreSQL database,
logical decoding must be enabled, in particular by setting `wal_level
= logical` in `postgresql.conf`.

Next we create a connector configuration file for Kafka Connect:

----
{
    "name": "example-1-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.dbname": "sourcedb",
        "database.hostname": "example.host.name",
        "database.password": "eHrkGrZL8mMJOFgToqqL",
        "database.port": "5432",
        "database.server.name": "metadb_example_1",
        "database.user": "dbuser",
        "plugin.name": "pgoutput",
        "snapshot.mode": "exported",
        "tasks.max": "1",
        "truncate.handling.mode": "include",
        "publication.autocreate.mode", "filtered"
        "heartbeat.interval.ms": "30000",
        "heartbeat.action.query": "UPDATE admin.heartbeat set last_heartbeat_ts = now();"
    }
}
----

It is recommended to use the connector configuration settings
`heartbeat.interval.ms` and `heartbeat.action.query` as above to avoid
spikes in disk space consumption within the source database.  (See the
Debezium PostgreSQL connector documentation for more details.)  The
`schemastopfilter` option of the `CREATE DATA SOURCE` command is used
to filter out the heartbeat table.

In the source database:

----
CREATE SCHEMA admin;

CREATE TABLE admin.heartbeat (last_heartbeat_ts timestamptz PRIMARY KEY);

INSERT INTO admin.heartbeat (last_heartbeat_ts) VALUES (now());
----

Then to create the connector:

----
curl -X POST -i -H "Accept: application/json" -H "Content-Type: application/json" \
     -d @connector.json https://kafka.connect.server/connectors
----

Note the `1` included in `name` and `database.server.name` in the
connector configuration.  This is suggested as a version number, which
can be incremented if the data stream needs to be resynchronized with
a new connector.

Metadb requires all streamed tables to have a primary key defined or a
replica identity of `FULL`.  Tables that do not meet this requirement
should be filtered out in the Debezium PostgreSQL connector
configuration by setting `schema.exclude.list` or
`table.exclude.list`.  Otherwise they will generate error messages in
the Metadb log.

The replication slot disk usage must be monitored, because under
certain error conditions it can grow too large and possibly fill up
the disk.  To show the disk usage (in the source database):

----
select slot_name, pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(),
    restart_lsn)) as replicationSlotLag, active from pg_replication_slots;
----

To drop the replication slot (in the source database) after deleting a
connector:

----
SELECT pg_drop_replication_slot('debezium');

DROP PUBLICATION dbz_publication;
----

=== Resynchronizing a data source

If a Kafka data stream fails and cannot be resumed, it may be
necessary to re-stream data to Metadb.  For example, a source database
may become unsynchronized with the analytic database, requiring a new
snapshot of the source database to be streamed.  Metadb can accept
re-streamed data in order to resynchronize with the source, using this
procedure:

1. Update the `topics` and `consumergroup` configuration settings for
   the new data stream.
+
[source]
----
ALTER DATA SOURCE example OPTIONS
    (SET topics '^metadb_example_2.*', SET consumergroup 'metadb_example_2_1');
----
+
*Do not restart the Metadb server but continue directly to Step 2.*

2. Stop the Metadb server and (before starting it again) "reset" the
   analytic database to mark current data as old.  This may take some
   time to run.
+
[source,bash]
----
metadb stop -D data

metadb reset -D data --source example
----

3. Start the Metadb server to begin streaming the data.

4. Once the new data have finished (or nearly finished) re-streaming,
   stop the Metadb server, and "clean" the analytic database to remove
   old data.
+
[source,bash]
----
metadb clean -D data --source example
----
+
Note that the metadb server currently does not give an indication that
it has finished re-streaming.  (Having the server report that initial
streaming or re-streaming has finished is a planned feature.)
+
The precise timing of when "metadb clean" is run is not important, but
*it must be run to remove redundant data and to complete the
resynchronization process*.  The only disadvantage of running it too
early is that data will appear to be missing until they are
re-streamed.

5. Start the server.
+
Until a failed stream is re-streamed by following the process above,
the analytic database may continue to be unsynchronized with the
source.

=== Configuring database users

To create a new database user account:

[source]
----
CREATE USER tiggy PASSWORD 'LZn2DCajcNHpGR3ZXWHD';
----

In addition to creating the user, this also creates a schema with the
same name as the user which can be used as a workspace.

To give the user access to all tables generated from a data source
(including tables not yet created):

[source]
----
AUTHORIZE SELECT
    ON ALL TABLES IN DATA SOURCE example
    TO tiggy;
----

.Note
****
[.text-center]
AUTHORIZE currently requires restarting the server before it will take
effect.
****
