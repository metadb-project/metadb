== Server administration

=== System requirements

==== Hardware requirements

* Metadb software:
** Architecture: x86-64 (AMD64). Metadb is tested primarily on x86-64. It may also run on ARM64, which is relatively new and may not be fully supported. Developers wishing to attempt work on ARM64 should proceed at their own risk.
** CPU: 2 cores
** Memory: 1 GB
** Local storage: 500 GB SSD
* PostgreSQL database:
** CPU: 4 cores
** Memory: 32 GB
** Storage: 2 TB SSD

==== Software requirements

* Operating system: https://www.debian.org[Debian] 12 or later. Metadb is tested primarily on Debian. It may also run on other operating systems. Developers wishing to attempt work on FreeBSD, MacOS, etc. should proceed at their own risk.
* https://www.postgresql.org/[PostgreSQL] 15 or later
* Required to build from source:
** https://golang.org/[Go] 1.21 or later
** goyacc (installation instructions below)
** https://www.colm.net/open-source/ragel/[Ragel] 6.10 or later
** https://gcc.gnu.org/[GCC C compiler] 9.3.0 or later

==== PostgreSQL configuration

* `autovacuum_analyze_scale_factor`: `0.01`
* `autovacuum_max_workers`: `1`
* `autovacuum_vacuum_cost_delay`: `0`
* `autovacuum_vacuum_insert_scale_factor`: `0.01`
* `autovacuum_vacuum_scale_factor`: `0.01`
* `checkpoint_timeout`: `3600`
* `cpu_tuple_cost`: `0.03`
* `default_statistics_target`: `1000`
* `effective_io_concurrency`: `1`
* `idle_in_transaction_session_timeout`: `60000`
* `idle_session_timeout`: `86400000`
* `maintenance_work_mem`: `1000000`
* `max_wal_size`: `10240`
* `shared_buffers`: `1250000`
* `statement_timeout`: `3600000`
* `work_mem`: `350000`

=== Building the software

First set the `GOPATH` environment variable to specify a path that can serve as
the build workspace for Go, e.g.:

[source,bash]
----
export GOPATH=$HOME/go
----

Then install goyacc:

[source,bash]
----
go install golang.org/x/tools/cmd/goyacc@master

export PATH=$PATH:$GOPATH/bin
----

Then to build the server:

[source,bash]
----
./build.sh
----

The `build.sh` script creates a `bin/` subdirectory and builds the `metadb`
executable there:

[source,bash]
----
./bin/metadb help
----

It is suggested that a `metadb` user be created and the server run by that
user, for example, in `/home/metadb`.

=== Metadb invocation

Just as `git` is invoked with the name of a subcommand as its first
argument (for example, `git add`, `git commit` or `git pull`) so the
`metadb` binary is invoked with the name of a subcommand as its first
argument (for example, `metadb start`, `metadb sync` or `metadb
migrate`). Different subcommands take different arguments: for
example, `metadb start` takes arguments to specify the data directory,
log file, etc.

Running `metadb` alone lists the available subcommands, and `metadb
help COMMAND` provides information about the command-line options used
by each subcommand.

=== Server configuration

Metadb makes use of local storage in a *data directory* which is created using
`metadb` with the `init` command.  In this example we will create the data
directory as `data/`:

[source,bash]
----
metadb init -D data
----

This will also create a file `metadb.conf` in the data directory with the
following structure:

[source,subs="verbatim,quotes"]
----
[main]
host = _host name of the PostgreSQL server_
port = _port number of the PostgreSQL server_
database = _database name_
superuser = _database superuser such as "postgres"_
superuser_password = _password of superuser_
systemuser = _database user that is the owner of the database_
systemuser_password = _password of systemuser_
sslmode = _should be set to "require" or stronger option_
----

Metadb does almost all its work as the specified system user. It
escalates to superuser only for a very few things such as creating users.

Metadb expects the database name to be `metadb` or to begin with `metadb_`;
otherwise it logs a warning message.

This file should be edited to add database connection parameters, for example:

[source,subs="verbatim,quotes"]
----
[main]
host = a.b.c
port = 5432
database = metadb
superuser = postgres
superuser_password = zpreCaWS7S79dt82zgvD
systemuser = mdbadmin
systemuser_password = ZHivGie5juxGJZmTObHU
sslmode = require
----

Metadb will assume that the database, superuser, and systemuser defined here
already exist; so they should be created before continuing.

=== Backups

*It is essential to make regular backups of Metadb and to test the backups.*

In general persistent data are stored in the database, and so https://www.postgresql.org/docs/current/backup.html[the database
should be backed up often].

The data directory contains the `metadb.conf` configuration file and is also
used for temporary storage.  The `metadb.conf` file should be backed up.

=== Upgrading from a previous version

To upgrade from any previous version of Metadb, stop the server (if running),
and then run the upgrade process in case changes are required:

----
metadb upgrade -D data
----

The upgrade process may, in some instances, take a significant amount of time
to run.  The database generally remains available to users during this period.

If no changes are needed, the process outputs:

----
metadb: "data" is up to date
----

=== Running the server

To start the server:

[source,bash]
----
nohup metadb start -D data -l metadb.log &
----

The `--memlimit` option can be used to set a soft memory limit (in GiB) if
needed, for example:

[source,bash]
----
nohup metadb start -D data -l metadb.log --memlimit 2 &
----

The server listens on port 8550 by default, but this can be set using the
`--port` option.  The `--debug` option enables verbose logging.

To stop the server:

[source,bash]
----
metadb stop -D data
----

Note that stopping or restarting the server may delay scheduled data updates or
cause them to restart.

The server can be set up to run with systemd via a file such as
`/etc/systemd/system/metadb.service`, for example:

[source,ini]
----
[Unit]
Description=Metadb
After=network.target remote-fs.target

[Service]
Type=simple
User=metadb
ExecStart=/bin/bash -ce "exec /home/metadb/bin/metadb start -D /home/metadb/data -l /home/metadb/metadb.log"
Restart=on-abort

[Install]
WantedBy=multi-user.target
----

Then:

----
systemctl enable metadb

systemctl start metadb
----

=== Connecting to the server

The PostgreSQL terminal-based client, `psql`, is used to connect to a Metadb
server that runs on the same host and listens on a specified port:

----
psql -X -h localhost -d metadb -p <port>
----

For example:

----
psql -X -h localhost -d metadb -p 8550
----

See *Reference > Statements* for commands that can be issued via `psql`.

It is not quite the case that a MetaDB server _is_ a Postgres server
with extensions.  It's close enough that we can and do use the `psql`
command-line client to communicate with MetaDB instead of the bespoke
legacy client `mdb`, but MetaDB does not proxy all unrecognized
commands through to the underlying Postgres.

=== Configuring a Kafka data source

==== Overview

Metadb currently supports reading https://kafka.apache.org/[Kafka]
messages in the format produced by the https://debezium.io/[Debezium]
https://debezium.io/documentation/reference/2.6/connectors/postgresql.html[PostgreSQL
connector] for Kafka Connect.  Configuration of Kafka, Kafka Connect,
Debezium, and PostgreSQL logical decoding is beyond the scope of this
documentation, but a few notes are included here.

Data flow in this direction:

1. A source PostgreSQL database (e.g. a FOLIO or ReShare database)
2. Kafka Connect/Debezium
3. Kafka
4. Metadb
5. The analytics database

==== Creating a connector

To allow capturing data changes in the source PostgreSQL database, logical
decoding has to be enabled, in particular by setting `wal_level = logical` in
the `postgresql.conf` for the source database.

Note that timeout settings in the source database such as
`idle_in_transaction_session_timeout` can cause the connector to fail, if a
timeout occurs while the connector is taking an initial snapshot of the
database.

Next we create a connector configuration file for Kafka Connect:

----
{
    "name": "sensor-1-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.dbname": "sourcedb",
        "database.hostname": "example.host.name",
        "database.password": "eHrkGrZL8mMJOFgToqqL",
        "database.port": "5432",
        "database.server.name": "metadb_sensor_1",
        "database.user": "dbuser",
        "plugin.name": "pgoutput",
        "snapshot.mode": "exported",
        "tasks.max": "1",
        "truncate.handling.mode": "include",
        "publication.autocreate.mode", "filtered"
        "heartbeat.interval.ms": "30000",
        "heartbeat.action.query": "UPDATE admin.heartbeat set last_heartbeat = now();"
    }
}
----

It is recommended to use the connector configuration settings
`heartbeat.interval.ms` and `heartbeat.action.query` as above to avoid spikes
in disk space consumption within the source database.  (See the Debezium
PostgreSQL connector documentation for more details.)  The `schemastopfilter`
option of the `CREATE DATA SOURCE` command is used to filter out the heartbeat
table.

In the source database:

----
CREATE SCHEMA admin;

CREATE TABLE admin.heartbeat (last_heartbeat timestamptz PRIMARY KEY);

INSERT INTO admin.heartbeat (last_heartbeat) VALUES (now());
----

Then to create the connector:

----
curl -X POST -i -H "Accept: application/json" -H "Content-Type: application/json" \
     -d @connector.json https://kafka.connect.server/connectors
----

Note the `1` included in `name` and `database.server.name` in the connector
configuration.  This is suggested as a version number, which can be incremented
if the data stream needs to be resynchronized with a new connector.

Metadb requires all streamed tables to have a primary key defined.  Tables that
do not meet this requirement should be filtered out in the Debezium PostgreSQL
connector configuration by setting `schema.exclude.list` or
`table.exclude.list`.  Otherwise they will generate error messages in the
Metadb log.

==== Monitoring replication

*The replication slot disk usage must be monitored, because under certain error
conditions it can grow too large and possibly fill up the disk.* To show the
disk usage (in the source database):

----
select slot_name, pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(),
    restart_lsn)) as replicationSlotLag, active from pg_replication_slots;
----

*It is recommended to allocate plenty of extra disk space in the source
database.*

==== Creating the data source

In Metadb, a data source is defined using the `CREATE DATA SOURCE` statement,
for example:

----
CREATE DATA SOURCE sensor TYPE kafka OPTIONS (
    brokers 'kafka:29092',
    topics '^metadb_sensor_1\.',
    consumergroup 'metadb_sensor_1_1',
    addschemaprefix 'sensor_',
    schemastopfilter 'admin'
);
----

==== Initial synchronization

[.aqua-background]#Metadb 1.2#
When a new data source is first configured using `CREATE DATA SOURCE`, Metadb
automatically puts the database into synchronizing mode, just as if `metadb
sync` had been run (see *Server administration > Resynchronizing a data
source*).  This has the effect of pausing periodic transforms and external SQL.
When the initial snapshot has finished streaming, the message "source snapshot
complete (deadline exceeded)" will be written to the log.  Then, to complete
this first synchronization, stop the Metadb server, and after that run `metadb
endsync`:

[source,bash]
----
metadb stop -D data

metadb endsync -D data --source sensor
----

Once "endsync" has finished running, start the Metadb server.

==== Deleting a connection

Sometimes a connection may have to be deleted and recreated (see *Server
administration > Resynchronizing a data source*).  After deleting a connection,
the replication slot and publication in the source database should be dropped
using:

----
SELECT pg_drop_replication_slot('debezium');

DROP PUBLICATION dbz_publication;
----

=== Resynchronizing a data source

If a Kafka data stream fails and cannot be resumed, it may be necessary to
re-stream a complete snapshot of the data to Metadb.  For example, a source
database may become unsynchronized with the analytic database, requiring a new
snapshot of the source database to be streamed.  Metadb can accept re-streamed
data in order to synchronize with the source, using the following procedure.

Note that during the synchronization process, the analytics database continues
to be available to users.  However, streaming updates will be slower than
usual, and there temporarily may be missing records (until they are
re-streamed) or "extra" records (recently deleted in the source database).
Also, periodic transforms and external SQL are paused during synchronization.

.Note
****
[.text-center]
The instructions below use the Metadb commands "sync" and "endsync".  In Metadb
versions before 1.2, these commands were called "reset" and "clean",
respectively.
****

1. Update the `topics` and `consumergroup` configuration settings for the new
   data stream.
+
[source]
----
ALTER DATA SOURCE sensor OPTIONS
    (SET topics '^metadb_sensor_2\.', SET consumergroup 'metadb_sensor_2_1');
----
+
*Do not restart the Metadb server but continue directly to Step 2.*

2. Stop the Metadb server and (before starting it again) run `metadb sync`.
   This may take some time to run.
+
[source,bash]
----
metadb stop -D data

metadb sync -D data --source sensor
----

3. Start the Metadb server to begin streaming the data.

4. Once the new data have finished (or nearly finished) re-streaming, stop the
   Metadb server, and run `metadb endsync` to remove any old data that have not
   been refreshed by the new data stream.
+
[source,bash]
----
metadb endsync -D data --source sensor
----
+
The timing of when "endsync" should be run is up to the admninistrator, but *it
must be run to complete the synchronization process*.  In most cases it will be
more convenient for users if "endsync" is run too late (delaying removal of
deleted records) rather than too early (removing records before they have been
restreamed).
+
Metadb detects when snapshot data are no longer being received, and then writes
"source snapshot complete (deadline exceeded)" to the log.  This generally
means it is a good time to run "endsync".

5. Start the server.
+
Until a failed stream is re-streamed by following the process above, the
analytic database may continue to be unsynchronized with the source.

=== Creating database users

To create a new database user account:

[source]
----
CREATE USER wegg WITH PASSWORD 'LZn2DCajcNHpGR3ZXWHD', COMMENT 'Silas Wegg';
----

In addition to creating the user, this also creates a schema with the same name
as the user.  The schema is intended as a workspace for the user.

.Recommendations:
* Each user account should be for an individual user and not shared by more
  than one person.
* Prefer user names of 3 to 8 characters in length.

By default the user does not have access to data tables.  To give the user
access to all tables generated from a data source (including tables not yet
created):

[source]
----
AUTHORIZE SELECT ON ALL TABLES IN DATA SOURCE sensor TO wegg;
----

.Note
****
[.text-center]
AUTHORIZE currently requires restarting the server before it will take
effect.
****

=== Administrative database changes

It is possible to make administrative-level changes directly in the underlying
PostgreSQL database, such as providing additional tables for users.  However,
the following guidelines should be followed strictly to avoid disrupting the
operation of Metadb:

1. No changes should be made to any database objects created by Metadb.  If it
should become necessary to make changes to the database objects at the request
of the Metadb maintainers, the server should be stopped first to prevent it
from operating with an out-of-date cache.  If changes are made to the database
objects inadvertently, the server should be stopped as soon as possible and not
started until the changes have been reversed.

2. Changes generally should be made while logged in as a new database user (not
a superuser) that is created specifically for that purpose.

3. Any new database objects should be created in a new schema that will not
coincide with a schema that may be created by Metadb.  This can be ensured by
always setting `addschemaprefix` in data source configurations and avoiding
names with those prefixes when creating a new schema.

4. Database views should not be created in the database.
