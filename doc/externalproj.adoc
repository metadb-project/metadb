== External projects

Two of the most important uses of Metadb are in streaming events from
FOLIO and ReShare databases.  Metadb has specific support for both
kinds of source, described in the following sections.

When configured for FOLIO, a Metadb instance typically handles data
for a single tenant, harvested using a single data source. If multiple
tenants are to be supported, then a separate Metadb instance runs for
each, and a separate PostgreSQL database holds each tenant's data.

When configured for ReShare (by the use of a source that uses the
`reshare` module), Metadb typically uses a single data source for the
whole consortium, but it combines data from multiple tenants into
aggregated tables.  Each record in these tables has an `\__origin`
field set to the name of the tenant from which it was derived.  The
ReShare data source does this by matching the start of the schema
names of incoming records against the set of origins that have been
defined by `CREATE DATA ORIGIN`.  For example if the source ReShare
database has `east_mod_rs.table1` and `west_mod_rs.table1`, the data
in Metadb will be in `mod_rs.table1` and each row's `__origin` will
contain the value `east` or `west` to indicate which tenant the data
came from.

(The Kafka stream feeding a ReShare data source is configured to
return records from all tenants rather than, as for a FOLIO data
source, a single tenant.  So configuring that stream is another
aspect that’s different for FOLIO and ReShare.)

The `__origin` field could be used for other things in non-ReShare
systems.  More generally, origin is intended as an alternative
category to source.  Source may be just an accident of how data are
collected, e.g. geographically in a sensor network.  Origin is a way
of tagging related data that come from different sources.  It's an
opaque string but it could potentially have a faceted structure like
`foo.bar.baz`.

One application of this is that it’s perfectly possible for a ReShare
consortium’s tenants to be hosted by multiple FOLIO instances.  We
would then set up MetaDB to use multiple sources, one per FOLIO
instance (or, more precisely, one per Kafka that is listening to a
FOLIO instance), and each of those sources would contribute records to
the same MetaDB ReShare schema but provide different sets of values
for those records’ `__origin` fields.




=== FOLIO

==== MARC transform

Metadb transforms MARC records from the tables `marc_records_lb` and
`records_lb` in schema `folio_source_record` to a tabular form which is stored
in a new table, `folio_source_record.marc__t`.  (These schema and table
names are hardwired rather than configurable, so that they can be used
in queries shared across the reporting community.)

Only MARC records considered to be current are transformed. Currency
is indicated by an established FOLIO-specific convention: the FOLIO
backing store record needs to have a `state` field with value `ACTUAL`
and the MARC record that is carried by that FOLIO record needs to have
a 999 field with both indicators set to `f` and whose `$i` subfield is
non-empty.

This transform updates the table `folio_source_record.marc__t` usually every
few hours or so.  The time of the most recent update can be retrieved from the
table `metadb.table_update`:

----
SELECT last_update
    FROM metadb.table_update
    WHERE schema_name = 'folio_source_record' AND table_name = 'marc__t';
----

The MARC transform stores partition tables in the schema `marctab`.  Users can
ignore this schema, as all data are accessible via `folio_source_record.marc__t`.

==== Derived tables

FOLIO "derived tables" are helper tables for commonly used joins,
etc. They are created and updated by the Scheduled Query facility
discussed above, automatically each day, usually at about 3:00
UTC by default.  The SQL code to create these tables is written by the
community and kept in
https://github.com/folio-org/folio-analytics/tree/main/sql_metadb/derived_tables[the
`sql_metadb/derived_tables/` area of the `folio-analytics`
repository]. The list of queries to run is held in
https://github.com/folio-org/folio-analytics/blob/main/sql_metadb/derived_tables/runlist.txt[`runlist.txt`].


Note that the derived tables are based on a periodic snapshot of data, and for
this reason they are generally not up-to-date.

==== Data model

FOLIO modules do not in general provide documentation for their internal data models, which Metadb
tables are based on, but they do have some data documentation for its "storage
module" WSAPIs which are roughly equivalent.  This documentation is collected at
`https://dev.folio.org/reference/api/`.  The names of most storage modules end
in `-storage`, but some modules use a different convention; for instance, the
storage module for users is `mod-users`.  (All module names begin with `mod-`.)

==== Migrating from LDP

This section contains notes related to migrating from LDP to Metadb.

===== Table names

Table names have changed and now are derived from FOLIO internal table
names. Whereas in LDP, all the tables were in a single schema, in
Metadb they are spread across multiple schemas, reflecting the schemas
in use in the FOLIO database and broadly indicative of the FOLIO
modules that manage them.

[%header,cols="8l,9l"]
|===
|LDP table
|Metadb table

|acquisition_method
|folio_orders.acquisition_method

|acquisitions_memberships
|folio_orders.acquisitions_unit_membership

|acquisitions_units
|folio_orders.acquisitions_unit

|audit_circulation_logs
|folio_audit.circulation_logs

|circulation_cancellation_reasons
|folio_circulation.cancellation_reason

|circulation_check_ins
|folio_circulation.check_in

|circulation_fixed_due_date_schedules
|folio_circulation.fixed_due_date_schedule

|circulation_loan_history
|folio_circulation.audit_loan

|circulation_loan_policies
|folio_circulation.loan_policy

|circulation_loans
|folio_circulation.loan

|circulation_patron_action_sessions
|folio_circulation.patron_action_session

|circulation_patron_notice_policies
|folio_circulation.patron_notice_policy

|circulation_request_policies
|folio_circulation.request_policy

|circulation_request_preference
|folio_circulation.user_request_preference

|circulation_requests
|folio_circulation.request

|circulation_scheduled_notices
|folio_circulation.scheduled_notice

|circulation_staff_slips
|folio_circulation.staff_slips

|configuration_entries
|folio_configuration.config_data

|course_copyrightstatuses
|folio_courses.coursereserves_copyrightstates

|course_courselistings
|folio_courses.coursereserves_courselistings

|course_courses
|folio_courses.coursereserves_courses

|course_coursetypes
|folio_courses.coursereserves_coursetypes

|course_departments
|folio_courses.coursereserves_departments

|course_processingstatuses
|folio_courses.coursereserves_processingstates

|course_reserves
|folio_courses.coursereserves_reserves

|course_roles
|folio_courses.coursereserves_roles

|course_terms
|folio_courses.coursereserves_terms

|email_email
|folio_email.email_statistics

|feesfines_accounts
|folio_feesfines.accounts

|feesfines_comments
|folio_feesfines.comments

|feesfines_feefineactions
|folio_feesfines.feefineactions

|feesfines_feefines
|folio_feesfines.feefines

|feesfines_lost_item_fees_policies
|folio_feesfines.lost_item_fee_policy

|feesfines_manualblocks
|folio_feesfines.manualblocks

|feesfines_overdue_fines_policies
|folio_feesfines.overdue_fine_policy

|feesfines_owners
|folio_feesfines.owners

|feesfines_payments
|folio_feesfines.payments

|feesfines_refunds
|folio_feesfines.refunds

|feesfines_transfer_criterias
|folio_feesfines.transfer_criteria

|feesfines_transfers
|folio_feesfines.transfers

|feesfines_waives
|folio_feesfines.waives

|finance_budgets
|folio_finance.budget

|finance_expense_classes
|folio_finance.expense_class

|finance_fiscal_years
|folio_finance.fiscal_year

|finance_fund_types
|folio_finance.fund_type

|finance_funds
|folio_finance.fund

|finance_group_fund_fiscal_years
|folio_finance.group_fund_fiscal_year

|finance_groups
|folio_finance.groups

|finance_ledgers
|folio_finance.ledger

|finance_transactions
|folio_finance.transaction

|inventory_alternative_title_types
|folio_inventory.alternative_title_type

|inventory_bound_with_part
|folio_inventory.bound_with_part

|inventory_call_number_types
|folio_inventory.call_number_type

|inventory_campuses
|folio_inventory.loccampus

|inventory_classification_types
|folio_inventory.classification_type

|inventory_contributor_name_types
|folio_inventory.contributor_name_type

|inventory_contributor_types
|folio_inventory.contributor_type

|inventory_electronic_access_relationships
|folio_inventory.electronic_access_relationship

|inventory_holdings
|folio_inventory.holdings_record

|inventory_holdings_note_types
|folio_inventory.holdings_note_type

|inventory_holdings_sources
|folio_inventory.holdings_records_source

|inventory_holdings_types
|folio_inventory.holdings_type

|inventory_identifier_types
|folio_inventory.identifier_type

|inventory_ill_policies
|folio_inventory.ill_policy

|inventory_instance_formats
|folio_inventory.instance_format

|inventory_instance_note_types
|folio_inventory.instance_note_type

|inventory_instance_relationship_types
|folio_inventory.instance_relationship_type

|inventory_instance_relationships
|folio_inventory.instance_relationship

|inventory_instance_statuses
|folio_inventory.instance_status

|inventory_instance_types
|folio_inventory.instance_type

|inventory_instances
|folio_inventory.instance

|inventory_institutions
|folio_inventory.locinstitution

|inventory_item_damaged_statuses
|folio_inventory.item_damaged_status

|inventory_item_note_types
|folio_inventory.item_note_type

|inventory_items
|folio_inventory.item

|inventory_libraries
|folio_inventory.loclibrary

|inventory_loan_types
|folio_inventory.loan_type

|inventory_locations
|folio_inventory.location

|inventory_material_types
|folio_inventory.material_type

|inventory_modes_of_issuance
|folio_inventory.mode_of_issuance

|inventory_nature_of_content_terms
|folio_inventory.nature_of_content_term

|inventory_service_points
|folio_inventory.service_point

|inventory_service_points_users
|folio_inventory.service_point_user

|inventory_statistical_code_types
|folio_inventory.statistical_code_type

|inventory_statistical_codes
|folio_inventory.statistical_code

|invoice_invoices
|folio_invoice.invoices

|invoice_lines
|folio_invoice.invoice_lines

|invoice_voucher_lines
|folio_invoice.voucher_lines

|invoice_vouchers
|folio_invoice.vouchers

|notes
|folio_notes.note

|organization_addresses
|folio_organizations.addresses

|organization_categories
|folio_organizations.categories

|organization_contacts
|folio_organizations.contacts

|organization_emails
|folio_organizations.emails

|organization_interfaces
|folio_organizations.interfaces

|organization_organizations
|folio_organizations.organizations

|organization_phone_numbers
|folio_organizations.phone_numbers

|organization_urls
|folio_organizations.urls

|patron_blocks_user_summary
|folio_patron_blocks.user_summary

|perm_permissions
|folio_permissions.permissions

|perm_users
|folio_permissions.permissions_users

|po_alerts
|folio_orders.alert

|po_lines
|folio_orders.po_line

|po_order_invoice_relns
|folio_orders.order_invoice_relationship

|po_order_templates
|folio_orders.order_templates

|po_pieces
|folio_orders.pieces

|po_purchase_orders
|folio_orders.purchase_order

|po_receiving_history
|(Not supported in Metadb)

|po_reporting_codes
|folio_orders.reporting_code

|srs_error
|folio_source_record.error_records_lb

|srs_marc
|folio_source_record.marc_records_lb

|srs_marctab
|folio_source_record.marc__t

|srs_records
|folio_source_record.records_lb

|template_engine_template
|folio_template_engine.template

|user_addresstypes
|folio_users.addresstype

|user_departments
|folio_users.departments

|user_groups
|folio_users.groups

|user_proxiesfor
|folio_users.proxyfor

|user_users
|folio_users.users
|===

===== Column names

The `data` column in LDP contains JSON objects.  In Metadb this column appears
as `jsonb` or in some cases `content`, matching the FOLIO internal column
names.

===== Data types

In Metadb, UUIDs generally have the `uuid` data type.  If a UUID has the `text`
data type preserved from the source data, it should be cast using `::uuid` in
queries.

Columns with the `json` data type in LDP have been changed to use the `jsonb`
data type in Metadb.

===== JSON queries

Querying JSON is very similar with Metadb as compared to LDP.  For clarity we
give a few examples below.

[discrete]
====== JSON source data

To select JSON data extracted from a FOLIO source, LDP supports:

----
SELECT data FROM user_groups;
----

In Metadb, this can be written as:

----
SELECT jsonb FROM folio_users.groups;
----

Or with easier to read formatting:

----
SELECT jsonb_pretty(jsonb) FROM folio_users.groups;
----

[discrete]
====== JSON fields: non-array data

For non-array JSON fields, extracting the data directly from JSON in LDP
usually takes the form:

----
SELECT json_extract_path_text(data, 'group') FROM user_groups;
----

The form recommended for Metadb is:

----
SELECT jsonb_extract_path_text(jsonb, 'group') FROM folio_users.groups;
----

[discrete]
====== JSON fields: array data

To extract JSON arrays, the syntax for Metadb is similar to LDP.  A lateral
join can be used with the function `jsonb_array_elements()` to convert the
elements of a JSON array to a set of rows, one row per array element.

For example, if the array elements are simple `text` strings:

----
CREATE TABLE instance_format_ids AS
SELECT id AS instance_id,
       instance_format_ids.jsonb #>> '{}' AS instance_format_id,
       instance_format_ids.ordinality
FROM folio_inventory.instance
    CROSS JOIN LATERAL jsonb_array_elements(jsonb_extract_path(jsonb, 'instanceFormatIds'))
        WITH ORDINALITY AS instance_format_ids (jsonb);
----

If the array elements are JSON objects:

----
CREATE TABLE holdings_notes AS
SELECT id AS holdings_id,
       jsonb_extract_path_text(notes.jsonb, 'holdingsNoteTypeId')::uuid
           AS holdings_note_type_id,
       jsonb_extract_path_text(notes.jsonb, 'note') AS note,
       jsonb_extract_path_text(notes.jsonb, 'staffOnly')::boolean AS staff_only,
       notes.ordinality
FROM folio_inventory.holdings_record
    CROSS JOIN LATERAL jsonb_array_elements(jsonb_extract_path(jsonb, 'notes'))
        WITH ORDINALITY AS notes (jsonb);
----

[discrete]
====== JSON fields as columns

LDP transforms simple, first-level JSON fields into columns, which can be
queried as, for example:

----
SELECT id, "group", "desc" FROM user_groups;
----

The Metadb equivalent of this query is:

----
SELECT id, "group", "desc" FROM folio_users.groups__t;
----

Note that the double quotation marks are needed here only because `group` and
`desc` are reserved words in SQL.  Alternatively, they can be removed if the
column names are prefixed with a table alias:

----
SELECT g.id, g.group, g.desc FROM folio_users.groups__t AS g;
----

Support for transforming subfields and arrays is planned in Metadb.

===== Migrating historical data from LDP

[.aqua-background]#Metadb 1.3#
Metadb can import legacy historical data from LDP.  The Metadb server must be
stopped while this process runs.  As an example:

----
metadb migrate -D data --ldpconf ldpconf.json --source folio
----

The file `ldpconf.json` is used to connect to the LDP database.  The output
looks something like:

----
Begin migration process? y
metadb: migrating: folio_audit.circulation_logs__: reading history.audit_circulation_logs where (updated < 2023-06-28 10:31:35.0556 +0000 UTC)
metadb: migrating: folio_audit.circulation_logs__: 3544356 records written
metadb: migrating: folio_circulation.audit_loan__: reading history.circulation_loan_history where (updated < 2023-06-28 03:34:57.32423 +0000 UTC)
metadb: migrating: folio_circulation.audit_loan__: 2201724 records written
metadb: migrating: folio_circulation.cancellation_reason__: reading history.circulation_cancellation_reasons where (updated < 2023-06-28 03:34:59.911506 +0000 UTC)
metadb: migrating: folio_circulation.cancellation_reason__: 22 records written
metadb: migrating: folio_circulation.check_in__: reading history.circulation_check_ins where (updated < 2023-06-28 11:31:38.628637 +0000 UTC)
metadb: migrating: folio_circulation.check_in__: 1095442 records written
metadb: migrating: folio_circulation.fixed_due_date_schedule__: reading history.circulation_fixed_due_date_schedules where (updated < 2023-07-04 10:31:46.899899 +0000 UTC)
metadb: migrating: folio_circulation.fixed_due_date_schedule__: 34 records written
metadb: migrating: folio_circulation.loan__: reading history.circulation_loans where (updated < 2023-06-28 03:34:57.932582 +0000 UTC)
metadb: migrating: folio_circulation.loan__: 1600346 records written
# (etc.)
----

Note that only records that LDP updated before a specific time stamp will be
imported.  This is because for each LDP table and corresponding Metadb table
there may be a range of times in which both LDP and Metadb contain historical
data.  In such cases, the Metadb data are preferred, and the import stops at
the point after which the two data sets would otherwise overlap.

Also note that JSON data contained in the imported records are not transformed
into columns.  (Also, there is presently no way to trigger this
transformation after the import is complete.)

Records imported using this process have their `__origin` column set to the
value `ldp`, which distinguishes them from other FOLIO data in Metadb.

==== Configuring Metadb for FOLIO

When creating a FOLIO data source, use the `module 'folio'` option, and set
`trimschemaprefix` to remove the tenant from schema names and `addschemaprefix`
to add a `folio_` prefix to the schema names.  For example:

----
CREATE DATA SOURCE folio TYPE kafka OPTIONS (
    module 'folio',
    trimschemaprefix 'tenantname_',
    addschemaprefix 'folio_',
    brokers 'kafka:29092',
    topics '^metadb_folio_1\.',
    consumergroup 'metadb_folio_1_1',
    schemastopfilter 'admin'
);
----

Specifying `module 'folio'` has multiple effects, including how
tenants are handled, where to find the scheduled queries to run, and
that MARC transformation is to be performed.  We trim the tenant-name
prefix from schema names because the Metadb database handles only a
single tenant.  (When we're interested in multiple FOLIO tenants, each
tenant's data goes into an entirely separate MetaDB database.)  We
conventionally add a `folio_` prefix to the schema names because we
generally want the option of using MetaDB for _all_ a library's data
analysis needs, not just those related to FOLIO.  For example, one
might also make a data-source for VuFind’s Solr database, and use it
to maintain tables with names like `vufind_inventory.items` and
`vufind_requests.requests`.

Note that there is nothing in the `CREATE DATA SOURCE` statement to
indicate which tenant's data is to be harvested. That information has
to be included in the configuration of the nominated Kafka topic, or
more precisely of the Debezium connector that publishes events to that
topic.

It is recommended to use a separate Kafka cluster, rather than the FOLIO Kafka
instance, until one has experience with administration of Kafka.

In the Debezium PostgreSQL connector configuration, the following exclusions
are suggested:

----
"schema.exclude.list": "public,.*_mod_login,.*_mod_pubsub,.*pubsub_config,supertenant_mod_.*,.*_mod_kb_ebsco_java,.*_mod_data_export_spring"
----
----
"table.exclude.list": ".*__system,.*_mod_agreements.alternate_resource_name,.*_mod_service_interaction.dashboard_access,.*_mod_agreements.availability_constraint,.*_mod_agreements\\.package_description_url,.*_mod_agreements\\.content_type,.*_mod_agreements\\.entitlement_tag,.*_mod_agreements\\.erm_resource_tag,.*_mod_agreements\\.string_template,.*_mod_agreements\\.string_template_scopes,.*_mod_agreements\\.templated_url,.*_mod_oai_pmh\\.instances,.*_mod_remote_storage\\.original_locations,.*_mod_remote_storage\\.item_notes,.*app_setting,.*alternate_name,.*databasechangelog,.*databasechangeloglock,.*directory_entry_tag,.*license_document_attachment,.*license_supp_doc,.*license_tag,.*log_entry_additional_info,.*subscription_agreement_supp_doc,.*subscription_agreement_document_attachment,.*subscription_agreement_ext_lic_doc,.*subscription_agreement_tag,.*tenant_changelog,.*tenant_changelog_lock,.*marc_indexers.*,.*rmb_internal.*,.*rmb_job.*,.*_mod_agreements\\.match_key,.*system_changelog"
----

Various tables are excluded for different reasons. Most of them are
omitted just because they are not of interest (e.g. pubsub state) but
data from some modules, e.g. `mod_login`, is omitted for security
reasons. It is up to individual libraries to tailor this exclusion
list to their requirements.

=== ReShare

==== Derived tables

ReShare "derived tables" are automatically updated once per day, usually at
about 3:00 UTC by default.

Note that the derived tables are based on a periodic snapshot of data, and for
this reason they are generally not up-to-date.

==== Configuring Metadb for ReShare

Before defining a ReShare data source, create a data origin for each
tenant in the consortium.  For example:

----
CREATE DATA ORIGIN tenant1;

CREATE DATA ORIGIN tenant2;

CREATE DATA ORIGIN tenant3;
----

.Note
****
[.text-center]
CREATE DATA ORIGIN currently requires restarting the server before it
will take effect.
****

Then use the `module 'reshare'` option when creating the data source, and set
`addschemaprefix` to add a `reshare_` prefix to the schema names:

----
CREATE DATA SOURCE reshare TYPE kafka OPTIONS (
    module 'reshare',
    addschemaprefix 'reshare_',
    brokers 'kafka:29092',
    topics '^metadb_reshare_1\.',
    consumergroup 'metadb_reshare_1_1',
    schemastopfilter 'admin'
);
----

We do not use `trimschemaprefix` when ingesting from ReShare, because
the `reshare` module uses the tenant names in the prefixes to choose
between the configured data origins, as described above.


Note that the order of commands is important: The initial set of data origins
should be created before the data source is created so that schema names of
incoming data will be processed correctly.  Later, whenever a new consortial
tenant is to be added, it should be defined in Metadb using `CREATE DATA
ORIGIN` (and the server restarted) before the tenant is added to ReShare.

In the Debezium PostgreSQL connector configuration, it is suggested that
credentials (`.+mod_login`), the public schema, the Okapi supertenant
(`supertenant_mod_.+`), and mod-pubsub data (`pubsub_config,.+_mod_pubsub`)
be excluded using the `schema.exclude.list` setting.

=== MARC transform for LDP

[.aqua-background]#Metadb 1.1#
The MARC transform that is built into Metadb can also be used with LDP (and LDLite).  A
command-line tool called `marct` is provided which is a drop-in replacement for
`ldpmarc`.

The system requirements are a subset of those for Metadb:

* Local storage: 500 GB
* Database storage: 500 GB
* Operating system: Linux
* https://www.postgresql.org/[PostgreSQL] 15 or later
* https://golang.org/[Go] 1.20 or later

To build `marct`:

----
mkdir -p bin && go build -o bin ./cmd/marct
----

which creates a `bin/` subdirectory and builds the `marct` executable there:

----
./bin/marct -h
----

In LDP, MARC data are read from the tables `public.srs_marc` and
`public.srs_records`, and the transformed output is written to the table
`public.srs_marctab`.

Typical usage is:

----
./bin/marct -D <datadir> -u <ldp_user>
----

where `datadir` is a LDP data directory containing `ldpconf.json`, and
`ldp1_user` is a LDP user to be granted `SELECT` privileges on the output
table.

For example:

----
./bin/marct -D data -u ldp
----

Note that `marct` only grants privileges for a single user.  If individual user
accounts are configured for LDP, a shell script can be used to grant privileges
to the users, for example:

----
users=/path/to/list/of/users.txt
for u in $( cat $users ); do
    psql -c "GRANT SELECT ON public.srs_marctab TO $u ;"
done
----

The first time `marct` runs, it will perform a "full update" of all of the MARC
records.  In subsequent runs, it will attempt to use "incremental update" to
update only records that have changed since the previous run, which can
dramatically reduce the running time if the number of changes is small.

However, if very many records have changed, it is possible that incremental
update may take longer than full update.  If it appears that an incremental
update will never finish, it should be canceled, and a full update should be
run once before resuming incremental updates.  This can be done by using the
`-f` command-line option, which disables incremental update and requires
`marct` to do a full update.

