== External projects

Two open-source communities, FOLIO and ReShare, use Metadb to support
analytics on their data.  Metadb has specific support for these two
types of data sources; it is referred to conceptually as the "folio"
module and the "reshare" module.  In the future it is intended that
this support will be moved into a plug-in or other modular system,
where it could be maintained and documented independently of the
Metadb project.

The following sections document Metadb features that are specific to
FOLIO and ReShare.

=== FOLIO

When configured for FOLIO, a Metadb instance typically manages data
for a single FOLIO tenant.  If multiple FOLIO tenants are to be
supported, then a separate Metadb instance and database should be set
up for each tenant.

==== MARC transform

Metadb transforms MARC records from the tables `marc_records_lb` and
`records_lb` in schema `folio_source_record` to a tabular form which
is stored in a new table, `folio_source_record.marc__t`.

Only MARC records considered to be "current" are transformed, where
current here is defined as having `state` = `'ACTUAL'` and a valid
identifier is present in `999 ff $i` (field `999`, both indicators
`f`, and subfield `$i`).

This transform updates the table `folio_source_record.marc__t` usually
every few hours or so.  The time of the most recent update can be
retrieved from the table `metadb.table_update`:

----
SELECT last_update
    FROM metadb.table_update
    WHERE schema_name = 'folio_source_record' AND table_name = 'marc__t';
----

The MARC transform stores partition tables in the schema `marctab`.
Users can ignore this schema, as all data are accessible via
`folio_source_record.marc__t`.

==== Derived tables

FOLIO "derived tables" are helper tables that provide commonly used
table joins, etc.  They are automatically updated once per day,
usually at about 3:00 UTC by default.

To enable these tables, set the server configuration parameter
`external_sql_folio` to the desired Git reference in the
folio-analytics repository.  For example:

----
ALTER SYSTEM SET external_sql_folio = 'refs/tags/v1.8.0';
----

Note that the derived tables are based on a periodic snapshot of data,
and for this reason they are generally not up-to-date.

==== Data model

FOLIO modules do not in general provide documentation for their
internal data models, which Metadb tables are based on, but they do
have some https://dev.folio.org/reference/api/[documentation for "storage module" WSAPIs] which are
roughly equivalent.  The names of most storage modules end in
`-storage`, but some modules use a different convention; for instance,
the storage module for users is `mod-users`.  (All module names begin
with `mod-`.)

==== Migrating from LDP

This section contains notes related to migrating from LDP to Metadb.

===== Table names

Table names have changed and now are derived from FOLIO internal table
names.  Note that unlike LDP, Metadb tables are spread across multiple
schemas, reflecting the naming structure of the source FOLIO database
in which tables are grouped by FOLIO module.

[%header,cols="8l,9l"]
|===
|LDP table
|Metadb table

|acquisition_method
|folio_orders.acquisition_method

|acquisitions_memberships
|folio_orders.acquisitions_unit_membership

|acquisitions_units
|folio_orders.acquisitions_unit

|audit_circulation_logs
|folio_audit.circulation_logs

|circulation_cancellation_reasons
|folio_circulation.cancellation_reason

|circulation_check_ins
|folio_circulation.check_in

|circulation_fixed_due_date_schedules
|folio_circulation.fixed_due_date_schedule

|circulation_loan_history
|folio_circulation.audit_loan

|circulation_loan_policies
|folio_circulation.loan_policy

|circulation_loans
|folio_circulation.loan

|circulation_patron_action_sessions
|folio_circulation.patron_action_session

|circulation_patron_notice_policies
|folio_circulation.patron_notice_policy

|circulation_request_policies
|folio_circulation.request_policy

|circulation_request_preference
|folio_circulation.user_request_preference

|circulation_requests
|folio_circulation.request

|circulation_scheduled_notices
|folio_circulation.scheduled_notice

|circulation_staff_slips
|folio_circulation.staff_slips

|configuration_entries
|folio_configuration.config_data

|course_copyrightstatuses
|folio_courses.coursereserves_copyrightstates

|course_courselistings
|folio_courses.coursereserves_courselistings

|course_courses
|folio_courses.coursereserves_courses

|course_coursetypes
|folio_courses.coursereserves_coursetypes

|course_departments
|folio_courses.coursereserves_departments

|course_processingstatuses
|folio_courses.coursereserves_processingstates

|course_reserves
|folio_courses.coursereserves_reserves

|course_roles
|folio_courses.coursereserves_roles

|course_terms
|folio_courses.coursereserves_terms

|email_email
|folio_email.email_statistics

|feesfines_accounts
|folio_feesfines.accounts

|feesfines_comments
|folio_feesfines.comments

|feesfines_feefineactions
|folio_feesfines.feefineactions

|feesfines_feefines
|folio_feesfines.feefines

|feesfines_lost_item_fees_policies
|folio_feesfines.lost_item_fee_policy

|feesfines_manualblocks
|folio_feesfines.manualblocks

|feesfines_overdue_fines_policies
|folio_feesfines.overdue_fine_policy

|feesfines_owners
|folio_feesfines.owners

|feesfines_payments
|folio_feesfines.payments

|feesfines_refunds
|folio_feesfines.refunds

|feesfines_transfer_criterias
|folio_feesfines.transfer_criteria

|feesfines_transfers
|folio_feesfines.transfers

|feesfines_waives
|folio_feesfines.waives

|finance_budgets
|folio_finance.budget

|finance_expense_classes
|folio_finance.expense_class

|finance_fiscal_years
|folio_finance.fiscal_year

|finance_fund_types
|folio_finance.fund_type

|finance_funds
|folio_finance.fund

|finance_group_fund_fiscal_years
|folio_finance.group_fund_fiscal_year

|finance_groups
|folio_finance.groups

|finance_ledgers
|folio_finance.ledger

|finance_transactions
|folio_finance.transaction

|inventory_alternative_title_types
|folio_inventory.alternative_title_type

|inventory_bound_with_part
|folio_inventory.bound_with_part

|inventory_call_number_types
|folio_inventory.call_number_type

|inventory_campuses
|folio_inventory.loccampus

|inventory_classification_types
|folio_inventory.classification_type

|inventory_contributor_name_types
|folio_inventory.contributor_name_type

|inventory_contributor_types
|folio_inventory.contributor_type

|inventory_electronic_access_relationships
|folio_inventory.electronic_access_relationship

|inventory_holdings
|folio_inventory.holdings_record

|inventory_holdings_note_types
|folio_inventory.holdings_note_type

|inventory_holdings_sources
|folio_inventory.holdings_records_source

|inventory_holdings_types
|folio_inventory.holdings_type

|inventory_identifier_types
|folio_inventory.identifier_type

|inventory_ill_policies
|folio_inventory.ill_policy

|inventory_instance_formats
|folio_inventory.instance_format

|inventory_instance_note_types
|folio_inventory.instance_note_type

|inventory_instance_relationship_types
|folio_inventory.instance_relationship_type

|inventory_instance_relationships
|folio_inventory.instance_relationship

|inventory_instance_statuses
|folio_inventory.instance_status

|inventory_instance_types
|folio_inventory.instance_type

|inventory_instances
|folio_inventory.instance

|inventory_institutions
|folio_inventory.locinstitution

|inventory_item_damaged_statuses
|folio_inventory.item_damaged_status

|inventory_item_note_types
|folio_inventory.item_note_type

|inventory_items
|folio_inventory.item

|inventory_libraries
|folio_inventory.loclibrary

|inventory_loan_types
|folio_inventory.loan_type

|inventory_locations
|folio_inventory.location

|inventory_material_types
|folio_inventory.material_type

|inventory_modes_of_issuance
|folio_inventory.mode_of_issuance

|inventory_nature_of_content_terms
|folio_inventory.nature_of_content_term

|inventory_service_points
|folio_inventory.service_point

|inventory_service_points_users
|folio_inventory.service_point_user

|inventory_statistical_code_types
|folio_inventory.statistical_code_type

|inventory_statistical_codes
|folio_inventory.statistical_code

|invoice_invoices
|folio_invoice.invoices

|invoice_lines
|folio_invoice.invoice_lines

|invoice_voucher_lines
|folio_invoice.voucher_lines

|invoice_vouchers
|folio_invoice.vouchers

|notes
|folio_notes.note

|organization_addresses
|folio_organizations.addresses

|organization_categories
|folio_organizations.categories

|organization_contacts
|folio_organizations.contacts

|organization_emails
|folio_organizations.emails

|organization_interfaces
|folio_organizations.interfaces

|organization_organizations
|folio_organizations.organizations

|organization_phone_numbers
|folio_organizations.phone_numbers

|organization_urls
|folio_organizations.urls

|patron_blocks_user_summary
|folio_patron_blocks.user_summary

|perm_permissions
|folio_permissions.permissions

|perm_users
|folio_permissions.permissions_users

|po_alerts
|folio_orders.alert

|po_lines
|folio_orders.po_line

|po_order_invoice_relns
|folio_orders.order_invoice_relationship

|po_order_templates
|folio_orders.order_templates

|po_pieces
|folio_orders.pieces

|po_purchase_orders
|folio_orders.purchase_order

|po_receiving_history
|(Not supported in Metadb)

|po_reporting_codes
|folio_orders.reporting_code

|srs_error
|folio_source_record.error_records_lb

|srs_marc
|folio_source_record.marc_records_lb

|srs_marctab
|folio_source_record.marc__t

|srs_records
|folio_source_record.records_lb

|template_engine_template
|folio_template_engine.template

|user_addresstypes
|folio_users.addresstype

|user_departments
|folio_users.departments

|user_groups
|folio_users.groups

|user_proxiesfor
|folio_users.proxyfor

|user_users
|folio_users.users
|===

===== Column names

The `data` column in LDP contains JSON objects.  In Metadb this column
appears as `jsonb` or in some cases `content`, matching the FOLIO
internal column names.

===== Data types

In Metadb, UUIDs generally have the `uuid` data type.  If a UUID has
the `text` data type preserved from the source data, it should be cast
using `::uuid` in queries.

Columns with the `json` data type in LDP have been changed to use the
`jsonb` data type in Metadb.

===== JSON queries

Querying JSON is very similar with Metadb as compared to LDP.  For
clarity we give a few examples below.

[discrete]
====== JSON source data

To select JSON data extracted from a FOLIO source, LDP supports:

----
SELECT data FROM user_groups;
----

In Metadb, this can be written as:

----
SELECT jsonb FROM folio_users.groups;
----

Or with easier to read formatting:

----
SELECT jsonb_pretty(jsonb) FROM folio_users.groups;
----

[discrete]
====== JSON fields: non-array data

For non-array JSON fields, extracting the data directly from JSON in
LDP usually takes the form:

----
SELECT json_extract_path_text(data, 'group') FROM user_groups;
----

The form recommended for Metadb is:

----
SELECT jsonb_extract_path_text(jsonb, 'group') FROM folio_users.groups;
----

[discrete]
====== JSON fields: array data

To extract JSON arrays, the syntax for Metadb is similar to LDP.  A
lateral join can be used with the function `jsonb_array_elements()` to
convert the elements of a JSON array to a set of rows, one row per
array element.

For example, if the array elements are simple `text` strings:

----
CREATE TABLE instance_format_ids AS
SELECT id AS instance_id,
       instance_format_ids.jsonb #>> '{}' AS instance_format_id,
       instance_format_ids.ordinality
FROM folio_inventory.instance
    CROSS JOIN LATERAL jsonb_array_elements(jsonb_extract_path(jsonb, 'instanceFormatIds'))
        WITH ORDINALITY AS instance_format_ids (jsonb);
----

If the array elements are JSON objects:

----
CREATE TABLE holdings_notes AS
SELECT id AS holdings_id,
       jsonb_extract_path_text(notes.jsonb, 'holdingsNoteTypeId')::uuid
           AS holdings_note_type_id,
       jsonb_extract_path_text(notes.jsonb, 'note') AS note,
       jsonb_extract_path_text(notes.jsonb, 'staffOnly')::boolean AS staff_only,
       notes.ordinality
FROM folio_inventory.holdings_record
    CROSS JOIN LATERAL jsonb_array_elements(jsonb_extract_path(jsonb, 'notes'))
        WITH ORDINALITY AS notes (jsonb);
----

[discrete]
====== JSON fields as columns

LDP transforms simple, first-level JSON fields into columns, which can
be queried as, for example:

----
SELECT id, "group", "desc" FROM user_groups;
----

The Metadb equivalent of this query is:

----
SELECT id, "group", "desc" FROM folio_users.groups__t;
----

Note that the double quotation marks are needed here only because
`group` and `desc` are reserved words in SQL.  Alternatively, they can
be removed if the column names are prefixed with a table alias:

----
SELECT g.id, g.group, g.desc FROM folio_users.groups__t AS g;
----

Support for transforming subfields and arrays is planned in Metadb.

===== Migrating historical data from LDP

Metadb can import legacy historical data from LDP.  The Metadb server
must be stopped while this process runs.  As an example:

----
metadb migrate -D data --ldpconf ldpconf.json --source folio
----

The file `ldpconf.json` is used to connect to the LDP database.  The
output looks something like:

----
Begin migration process? y
metadb: migrating: folio_audit.circulation_logs__: reading history.audit_circulation_logs where (updated < 2023-06-28 10:31:35.0556 +0000 UTC)
metadb: migrating: folio_audit.circulation_logs__: 3544356 records written
metadb: migrating: folio_circulation.audit_loan__: reading history.circulation_loan_history where (updated < 2023-06-28 03:34:57.32423 +0000 UTC)
metadb: migrating: folio_circulation.audit_loan__: 2201724 records written
metadb: migrating: folio_circulation.cancellation_reason__: reading history.circulation_cancellation_reasons where (updated < 2023-06-28 03:34:59.911506 +0000 UTC)
metadb: migrating: folio_circulation.cancellation_reason__: 22 records written
metadb: migrating: folio_circulation.check_in__: reading history.circulation_check_ins where (updated < 2023-06-28 11:31:38.628637 +0000 UTC)
metadb: migrating: folio_circulation.check_in__: 1095442 records written
metadb: migrating: folio_circulation.fixed_due_date_schedule__: reading history.circulation_fixed_due_date_schedules where (updated < 2023-07-04 10:31:46.899899 +0000 UTC)
metadb: migrating: folio_circulation.fixed_due_date_schedule__: 34 records written
metadb: migrating: folio_circulation.loan__: reading history.circulation_loans where (updated < 2023-06-28 03:34:57.932582 +0000 UTC)
metadb: migrating: folio_circulation.loan__: 1600346 records written
# (etc.)
----

Note that only records that LDP updated before a specific time stamp
will be imported.  This is because for each LDP table and
corresponding Metadb table there may be a range of times in which both
LDP and Metadb contain historical data.  In such cases, the Metadb
data are preferred, and the import stops at the point after which the
two data sets would otherwise overlap.

Also note that JSON data contained in the imported records are not
transformed into columns.

Records imported using this process have their `__origin` column set
to the value `ldp`, which distinguishes them from other FOLIO data in
Metadb.

==== Configuring Metadb for FOLIO

When creating a FOLIO data source, use the `module 'folio'` option,
and set `trim_schema_prefix` to remove the tenant from schema names
and `add_schema_prefix` to add a `folio_` prefix to the schema names.
For example:

----
CREATE DATA SOURCE folio TYPE kafka OPTIONS (
    module 'folio',
    trim_schema_prefix 'tenantname_',
    add_schema_prefix 'folio_',
    brokers 'kafka:29092',
    topics '^metadb_folio_1\.',
    consumer_group 'metadb_folio_1_1',
    schema_stop_filter 'admin'
);
----

Specifying `module 'folio'` has multiple effects including how tenants
are handled, where to find the derived tables, and that MARC
transformation is to be performed.  We trim the tenant-name prefix
from schema names because the Metadb database handles only a single
tenant.  We add the `folio_` prefix as a namespace to allow for other
(non-FOLIO) library data to be imported into the database as well, in
order to support cross-domain analytics.

Note that the `CREATE DATA SOURCE` statement does not control which
tenant's data are to be streamed.  The tenant is typically selected in
the configuration of the Debezium connector.

It is recommended to use a separate Kafka cluster, rather than the
FOLIO Kafka instance, until one has experience with administration of
Kafka.

In the Debezium PostgreSQL connector configuration, the following
exclusions are suggested:

----
"schema.exclude.list": "public,.*_mod_login,.*_mod_pubsub,.*pubsub_config,supertenant_mod_.*,.*_mod_kb_ebsco_java,.*_mod_data_export_spring"
----
----
"table.exclude.list": ".*__system,.*_mod_agreements.alternate_resource_name,.*_mod_service_interaction.dashboard_access,.*_mod_agreements.availability_constraint,.*_mod_agreements\\.package_description_url,.*_mod_agreements\\.content_type,.*_mod_agreements\\.entitlement_tag,.*_mod_agreements\\.erm_resource_tag,.*_mod_agreements\\.string_template,.*_mod_agreements\\.string_template_scopes,.*_mod_agreements\\.templated_url,.*_mod_oai_pmh\\.instances,.*_mod_remote_storage\\.original_locations,.*_mod_remote_storage\\.item_notes,.*app_setting,.*alternate_name,.*databasechangelog,.*databasechangeloglock,.*directory_entry_tag,.*license_document_attachment,.*license_supp_doc,.*license_tag,.*log_entry_additional_info,.*subscription_agreement_supp_doc,.*subscription_agreement_document_attachment,.*subscription_agreement_ext_lic_doc,.*subscription_agreement_tag,.*tenant_changelog,.*tenant_changelog_lock,.*marc_indexers.*,.*rmb_internal.*,.*rmb_job.*,.*_mod_agreements\\.match_key,.*system_changelog"
----

Tables can be excluded for various reasons.  Most of the tables
filtered above are excluded because they are not of interest for
analytics (e.g. pubsub state), but data from some modules,
e.g. `mod_login`, are omitted for security reasons.  It is up to
individual libraries to tailor this exclusion list to their
requirements.

=== ReShare

When configured for ReShare, a Metadb instance typically manages data
for a consortium using a single data source.  ReShare "tenants" are
not separated from each other in the usual sense; they each represent
a consortium member and together form a single consortial tenant.  For
this reason, all of the ReShare tenants are streamed to a single
Metadb instance, unlike FOLIO where a Metadb instance only handles a
single FOLIO tenant.

The data from consortium members are combined into aggregated tables,
with the `__origin` column set to the name of the ReShare tenant
(i.e. consortium member).  The `CREATE DATA ORIGIN` command defines
the set of known origins, and schema names in the source database are
assumed to begin with a prefix that identifies the origin (because
this is how FOLIO and ReShare store tenant data).  For example
`west_mod_rs.table1` has the prefix `west` which is taken to be the
origin if previously defined by `CREATE DATA ORIGIN`.

==== Derived tables

ReShare "derived tables" are automatically updated once per day,
usually at about 3:00 UTC by default.

To enable these tables, set the server configuration parameter
`external_sql_reshare` to the desired Git reference in the
reshare-analytics repository.  For example:

----
ALTER SYSTEM SET external_sql_reshare = 'refs/tags/20230912004531';
----

Note that the derived tables are based on a periodic snapshot of data,
and for this reason they are generally not up-to-date.

==== Configuring Metadb for ReShare

Before defining a ReShare data source, create a data origin for each
ReShare tenant (i.e. each member of the consortium).  For example:

----
CREATE DATA ORIGIN tenant1;

CREATE DATA ORIGIN tenant2;

CREATE DATA ORIGIN tenant3;
----

NOTE: CREATE DATA ORIGIN currently requires restarting the server
before it will take effect.

Then use the `module 'reshare'` option when creating the data source,
and set `add_schema_prefix` to add a `reshare_` prefix to the schema
names:

----
CREATE DATA SOURCE reshare TYPE kafka OPTIONS (
    module 'reshare',
    add_schema_prefix 'reshare_',
    brokers 'kafka:29092',
    topics '^metadb_reshare_1\.',
    consumer_group 'metadb_reshare_1_1',
    schema_stop_filter 'admin'
);
----

Note that unlike with FOLIO, here we do not use `trim_schema_prefix`,
because the "reshare" module uses the tenant name in the prefix to
choose a configured data origin, as described above.

Note that the order of commands is important: The initial set of data
origins should be created before the data source is created so that
schema names of incoming data will be processed correctly.  Later,
whenever a new consortial tenant is to be added, it should be defined
in Metadb using `CREATE DATA ORIGIN` (and the server restarted) before
the tenant is added to ReShare.

In the Debezium PostgreSQL connector configuration, it is suggested
that credentials (`.+mod_login`), the public schema, the Okapi
supertenant (`supertenant_mod_.+`), and mod-pubsub data
(`pubsub_config,.+_mod_pubsub`) be excluded using the
`schema.exclude.list` setting.

=== MARC transform for LDP/LDLite

The MARC transform (currently included in Metadb) can also be used
with LDP and LDLite.  A command-line tool called `marct` is provided
which is a drop-in replacement for `ldpmarc`.

The system requirements are a subset of those for Metadb:

* Local storage: 500 GB
* Database storage: 500 GB
* Operating system: Linux
* https://www.postgresql.org/[PostgreSQL] 15 or later
* https://golang.org/[Go] 1.20 or later

To build `marct`:

----
mkdir -p bin && go build -o bin ./cmd/marct
----

which creates a `bin/` subdirectory and builds the `marct` executable
there:

----
./bin/marct -h
----

In LDP, MARC data are read from the tables `public.srs_marc` and
`public.srs_records`, and the transformed output is written to the
table `public.srs_marctab`.

Typical usage is:

----
./bin/marct -D <datadir> -u <ldp_user>
----

where `datadir` is a LDP data directory containing `ldpconf.json`, and
`ldp1_user` is a LDP user to be granted `SELECT` privileges on the
output table.

For example:

----
./bin/marct -D data -u ldp
----

Note that `marct` only grants privileges for a single user.  If
individual user accounts are configured for LDP, a shell script can be
used to grant privileges to the users, for example:

----
users=/path/to/list/of/users.txt
for u in $( cat $users ); do
    psql -c "GRANT SELECT ON public.srs_marctab TO $u ;"
done
----

The first time `marct` runs, it will perform a "full update" of all of
the MARC records.  In subsequent runs, it will attempt to use
"incremental update" to update only records that have changed since
the previous run, which can dramatically reduce the running time if
the number of changes is small.

However, if very many records have changed, it is possible that
incremental update may take longer than full update.  If it appears
that an incremental update will never finish, it should be canceled,
and a full update should be run once before resuming incremental
updates.  This can be done by using the `-f` command-line option,
which disables incremental update and requires `marct` to do a full
update.

